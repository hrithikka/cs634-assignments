{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm - Spring 2023\n",
    "\n",
    "## Problem 1: Take-at-home  (45 points total)\n",
    "\n",
    "You are applying for a position at the data science team of USDA and you are given data associated with determining appropriate parasite treatment of canines. The suggested treatment options are determined based on a **logistic regression** model that predicts if the canine is infected with a parasite. \n",
    "\n",
    "The data is given in the site: https://data.world/ehales/grls-parasite-study/workspace/file?filename=CBC_data.csv  and more specifically in the CBC_data.csv file. Login using you University Google account to access the data and the description that includes a paper on the study (**you dont need to read the paper to solve this problem**). Your target variable $y$ column is titled `parasite_status`. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Feature Engineering (5 points)\n",
    "\n",
    "In this step you outline the following as potential features (this is a limited example - we can have many features as in your programming exercise below). \n",
    "\n",
    "Write the posterior probability expressions for logistic regression for the problem you are given to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y=1| \\mathbf{x}, \\mathbf w)=$$ \n",
    "\n",
    "$$p(y=0| \\mathbf{x}, \\mathbf w)=$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Question 2 - Decision Boundary (5 points)\n",
    "\n",
    "Write the expression for the decision boundary assuming that $p(y=1)=p(y=0)$. The decision boundary is the line that separates the two classes.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Question 3 - Loss function (5 points)\n",
    "\n",
    "Write the expression of the loss as a function of $\\mathbf w$ that makes sense for you to use in this problem. \n",
    "\n",
    "NOTE: The loss will be a function that will include this function: \n",
    "\n",
    "$$\\sigma(a) = \\frac{1}{1+e^{-a}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L_{CE} = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 4 - Gradient (5 points)\n",
    "\n",
    "Write the expression of the gradient of the loss with respect to the parameters - show all your work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\nabla_\\mathbf w L_{CE} = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 - Imbalanced dataset (10 points)\n",
    "\n",
    "You are now told that in the dataset  \n",
    "\n",
    "$$p(y=0) >> p(y=1)$$\n",
    "\n",
    "Can you comment if the accuracy of Logistic Regression will be affected by such imbalance?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 6 - SGD (15 points)\n",
    "\n",
    "The interviewer was impressed with your answers and wants to test your programming skills. \n",
    "\n",
    "1. Use the dataset to train a logistic regressor that will predict the target variable $y$. \n",
    "\n",
    " 2. Report the harmonic mean of precision (p) and recall (r) i.e the  [metric called $F_1$ score](https://en.wikipedia.org/wiki/F-score) that is calculated as shown below using a test dataset that is 20% of each group. Plot the $F_1$ score vs the iteration number  $t$. \n",
    "\n",
    "$$F_1 = \\frac{2}{r^{-1} + p^{-1}}$$\n",
    "\n",
    "Your code includes hyperparameter optimization of the learning rate and mini batch size. Please learn about cross validation which is a splitting strategy for tuning models [here](https://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "\n",
    "You are allowed to use any library you want to code this problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 - Feature Engineering (5 points)**\n",
    "\n",
    "In this step, you outline the following as potential features (this is a\n",
    "limited example - we can have many features as in your programming\n",
    "exercise below).\n",
    "\n",
    "Write the posterior probability expressions for logistic regression for\n",
    "the problem you are given to solve.\n",
    "\n",
    "𝑝(𝑦=1\\|𝐱,𝐰)=\n",
    "\n",
    "𝑝(𝑦=0\\|𝐱,𝐰)=\n",
    "\n",
    "Answer:\n",
    "\n",
    "𝑝(𝑦=1\\|𝐱,𝐰) = 1 / (1 + exp(-𝐰𝑇𝐱))\n",
    "\n",
    "𝑝(𝑦=0\\|𝐱,𝐰) = exp(-𝐰𝑇𝐱) / (1 + exp(-𝐰𝑇𝐱))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 - Decision Boundary (5 points)**\n",
    "\n",
    "Write the expression for the decision boundary assuming that\n",
    "𝑝(𝑦=1)=𝑝(𝑦=0)\n",
    "\n",
    ". The decision boundary is the line that separates the two classes.\n",
    "\n",
    "Type Markdown and LaTeX: 𝛼2\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Assuming that 𝑝(𝑦=1)=𝑝(𝑦=0), the decision boundary for logistic\n",
    "regression can be expressed as:\n",
    "\n",
    "<img\n",
    "src=\"DM.jpg\"\n",
    "style=\"width:3.64634in;height:1.61481in\" />\n",
    "\n",
    "Therefore, the decision boundary is the set of all \\$\\mathbf{x}\\$ that\n",
    "satisfy \\$p(y=1 \\mid \\mathbf{x}, \\boldsymbol{\\theta}) = p(y=0 \\mid\n",
    "\\mathbf{x}, \\boldsymbol{\\theta})\\$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 - Loss function (5 points)**\n",
    "\n",
    "Write the expression of the loss as a function of 𝐰 that makes sense for\n",
    "you to use in this problem.\n",
    "\n",
    "NOTE: The loss will be a function that will include this function:\n",
    "\n",
    "𝜎(𝑎)=1/1+𝑒−𝑎\n",
    "\n",
    "𝐿𝐶𝐸=\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The binary cross-entropy loss function can be used for binary logistic regression. The loss function is written as follows:\n",
    "\n",
    "\n",
    "$L CE(mathbfw) = -frac1Nsum i=1Ny ilogleft(sigma(mathbfwTmathbfx i)right) + (1-y i)logleft(1-sigma(mathbfwTmathbfx i)right)$\n",
    "\n",
    "where $mathbfw$ is the weight vector, $mathbfx i$ is the feature vector for the $i$-th example, $y i$ is the true label (either 0 or 1), $sigma(cdot)$ is the sigmoid function defined as $sigma(z) = frac11 + e-z$, and $N$ is the total number of instances.\n",
    "\n",
    "It is worth noting that the loss function penalizes the model for generating inaccurate predictions by raising the loss value. In contrast, as the model improves in predicting the proper labels for the samples, the loss reduces. The purpose of training the model is to minimize the loss function's value with respect to the weight vector $mathbfw$ that can be achieved using optimization techniques such as gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 - Gradient**\n",
    "\n",
    "Write the expression of the gradient of the loss with respect to the\n",
    "parameters - show all your work.\n",
    "\n",
    "∇𝐰𝐿𝐶𝐸=\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "We can start by calculating the derivative of the loss function with\n",
    "respect to the activation function:\n",
    "\n",
    "\\$\\$\\frac{\\partial L\\_{CE}}{\\partial a} = \\frac{1-y}{1-\\sigma(a)} -\n",
    "\\frac{y}{\\sigma(a)}\\$\\$\n",
    "\n",
    "where \\$\\sigma(a) = \\frac{1}{1 + e^{-a}}\\$ is the sigmoid function.\n",
    "\n",
    "Next, we can use the chain rule to calculate the derivative of the loss\n",
    "with respect to the weights:\n",
    "\n",
    "\\$\\$\\frac{\\partial L\\_{CE}}{\\partial w_j} = \\frac{\\partial\n",
    "L\\_{CE}}{\\partial a}\\frac{\\partial a}{\\partial w_j} =\n",
    "\\left(\\frac{1-y}{1-\\sigma(a)} - \\frac{y}{\\sigma(a)}\\right)x_j\\$\\$\n",
    "\n",
    "where \\$x_j\\$ is the j-th feature in our dataset.\n",
    "\n",
    "So the gradient of the loss with respect to the weights is:\n",
    "\n",
    "\\$\\$\\nabla_w L\\_{CE} = \\left(\\frac{1-y}{1-\\sigma(a)} -\n",
    "\\frac{y}{\\sigma(a)}\\right)\\mathbf{x}\\$\\$\n",
    "\n",
    "where \\$\\mathbf{x}\\$ is the feature vector (i.e. a row in our dataset).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 - Imbalanced dataset (10 points)**\n",
    "\n",
    "You are now told that in the dataset\n",
    "\n",
    "𝑝(𝑦=0)\\>\\>𝑝(𝑦=1)\n",
    "\n",
    "Can you comment if the accuracy of Logistic Regression will be affected\n",
    "by such imbalance?\n",
    "\n",
    "Type Markdown and LaTeX: 𝛼2\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Yes, the accuracy of Logistic Regression will be affected by such\n",
    "imbalance. In an imbalanced dataset, the algorithm will tend to predict\n",
    "the majority class more often, resulting in a high accuracy for the\n",
    "majority class but poor accuracy for the minority class. In this case,\n",
    "since the probability of y=0 is much higher than the probability of y=1,\n",
    "the algorithm will tend to predict y=0 more often, leading to poor\n",
    "accuracy for y=1. Therefore, in imbalanced datasets, accuracy may not be\n",
    "a good measure of model performance, and other metrics such as\n",
    "precision, recall, F1-score, or area under the ROC curve (AUC) should be\n",
    "used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6 - SGD (15 points)**\n",
    "\n",
    "The interviewer was impressed with your answers and wants to test your\n",
    "programming skills.\n",
    "\n",
    "Use the dataset to train a logistic regressor that will predict the\n",
    "target variable 𝑦.\n",
    "\n",
    "Report the harmonic mean of precision (p) and recall (r) i.e the metric\n",
    "called 𝐹1\n",
    "\n",
    "score that is calculated as shown below using a test dataset that is 20%\n",
    "of each group. Plot the 𝐹1\n",
    "\n",
    "score vs the iteration number 𝑡\n",
    "\n",
    ".\n",
    "\n",
    "𝐹1=2/𝑟−1+𝑝−1\n",
    "\n",
    "Your code includes hyperparameter optimization of the learning rate and\n",
    "mini batch size. Please learn about cross validation which is a\n",
    "splitting strategy for tuning models here.\n",
    "\n",
    "You are allowed to use any library you want to code this problem.\n",
    "\n",
    "Answer\n",
    "\n",
    "Here’s the code to implement it in Python using scikit-learn library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 2395 but corresponding boolean dimension is 599",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m missing_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(y_test)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# remove samples with missing labels from X_train and y_train\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mmissing_labels\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     23\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_train[\u001b[38;5;241m~\u001b[39mmissing_labels]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# drop samples with missing values in y_train\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#X_train = X_train[mask]\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#y_train = y_train[mask]\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# set up a pipeline with an imputer transformer and a logistic regression model\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 2395 but corresponding boolean dimension is 599"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load the dataset\n",
    "data = np.genfromtxt('CBC_data.csv', delimiter=',', skip_header=True)\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[:, :-1], data[:, -1], test_size=0.2)\n",
    "\n",
    "# impute missing values in X_train and X_test\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "# identify samples with missing labels\n",
    "missing_labels = np.isnan(y_test)\n",
    "\n",
    "# remove samples with missing labels from X_train and y_train\n",
    "X_train = X_train[~missing_labels]\n",
    "y_train = y_train[~missing_labels]\n",
    "\n",
    "\n",
    "\n",
    "# drop samples with missing values in y_train\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# set up a pipeline with an imputer transformer and a logistic regression model\n",
    "pipe = Pipeline([\n",
    "    ('model', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# train the model on the training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test data\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# print the F1 score\n",
    "print('F1 score:', f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "62556f7a043365a66e0918c892755cfafede529a87e97207556f006a109bade4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
